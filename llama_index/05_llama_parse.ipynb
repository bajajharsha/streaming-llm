{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
      "Started parsing the file under job_id 34d03026-72f5-4275-93cf-961747be6331\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=\"llx-iradePic1OvGNrujnQEuEvhueSdieVLWAz4cUSs7C6Ccnh67\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "file_extractor = {\".jpeg\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../data\", file_extractor=file_extractor\n",
    ").load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "embed_model=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    api_key=\"AIzaSyDYn0Vm5GFCk6g2WWtSPoejyYtl8rmaPpw\",  # uses GOOGLE_API_KEY env var by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "\n",
    "# Configure settings globally\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "index.storage_context.persist(persist_dir=\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text mentions deterministic and stochastic methods as decoding strategies used in large language models to select words during text generation.  Additionally, typical sampling for optimizing information content and speculative sampling for boosting inference speed are also described.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"What are the decoding strategies?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
